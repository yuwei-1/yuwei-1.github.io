<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" type="text/css" href="style/style.css">
    <meta charset="UTF-8">
    <div class="navbar">
      <a href="index.html" target="_self">Home</a>
      <a href="projects.html" target="_self">Projects</a>
    </div>
    <title>Yuwei Zhu</title>
</head>

<body id="backdrop">
  <h1 class="text" id="main-title">PERSONAL PROJECTS</h1>
    <div class="center project top">
      <button type="button" class="collapsible text">Reinforcement Learning for Market Prediction</button>
      <div class="hidden project">
        <div>
          <button type="button" class="sub-collapsible text">Initial Work</button>
          <div class="hidden text paragraph margintop">
            <p>
              One possible strategy for trading the financial markets is utilising reinforcement learning algorithms, and frame the
              trading task as a "game", where the agent collects rewards for good trades and negative rewards for bad trades.
              This particular personal project does a deep dive into common reinforcement learning strategies, before
              applying them to the stock market.
              <br></br>
              Before diving straight into a trading environment, we will sanity check our implementation and tune the function approximator (neural network)
              using the well known cartpole game provided by OpenAI Gym/Gymnasium (fig. 1). The code used in this project can be found 
              <a href="https://github.com/yuwei-1/Market-Prediction-Research" target="_blank">here</a>. 
              Included in the repository is a full set of python unittests which check basic functionality of each of the implemented models to ensure
              that the model is working as intended. 
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/env.png" class="images">
                  <figcaption>
                    Figure 1: Cartpole environment
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/dqn_algorithm.png" class="images">
                  <figcaption>
                    Figure 2: Description of the DQN algorithm
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              <br></br>
              We will be utilizing the Q-learning algorithm (fig. 2), which is an off-policy algorithm (which means it chooses actions in 
              a different manner compared to the policy the agent is trying to learn) which uses a temporal difference TD(0) update.
              When using a neural network to approximate the action-value function (usually denoted with Q(s,a)), the network is 
              commonly referred to as DQN. The DQN can be augmented in certain ways to produce other variants which can excel under
              certain circumstances; namely Double DQN, DynaQ, DynaQ+ and even dueling DQNs (which will not be covered in this project). 
              We will be testing the efficacy of these models on the aforementioned environment, before eventually moving to predicting optimal trades. 

              The DQN itself is a very powerful model which was demonstrated to play many Atari games with good results and even achieving super human play
              in some games. The Q-learning algorithm can be applied to both tabular and continuous environments, by estimating a value for each possible
              action and state combo. The Q-learning update is mathematically proven to converge to the optimal policy under certain conditions; namely that
              each state is visited an infinite number of times. However, as we can only train for finite time periods, sometimes the Q-learning agent
              can get stuck in a local minima, and one commonly known issue is overestimation. In fact, when performing a deep dive on the DQN using a 
              basic toy environment which always remains in the same state, with two possible actions, 0 and 1 - where 1 always yields a reward of 1, whereas
              the action zero permanently grants zero reward, I observed that the estimated value for action zero often gets overestimated during training, despite
              the model never encountering a reward for that action; this is because of the greedy term which takes the maximum value of the value function for the 
              next state.
              <br></br>
              Here we show the reward versus time plots for the DQN itself, and it can be seen that the model is highly unstable and struggles to reach
              the optimal policy, even after 5000 episodes of training.
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/single_dqn_cartpole.png" class="images">
                  <figcaption>
                    Figure 3: single DQN run 1
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/single_dqn_cartpole_ours.png" class="images">
                  <figcaption>
                    Figure 4: single DQN run 2
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              We can augment the DQN model such that we duplicate the Q-network to form a policy network and a target network. The intuition behind this 
              construction is to mitigate the overestimation problem and the "moving target" problem. By fixing the one network and updating the other, we can fix the policy while updating
              which makes the training much more stable. Obviously the fixed network prediction will be bad initially, but it is updated periodically to match
              the network that is being updated. Additionally, we experiment with hard and soft updates, which are complete updates at intervals, or small 
              interpolations at each time step.
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/double_dqn_hard_cartpole_ours.png" class="images">
                  <figcaption>
                    Figure 5: Double DQN with hard updates every 500 steps
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/double_dqn_cartpole_ours.png" class="images">
                  <figcaption>
                    Figure 6: Double DQN with soft updates - interpolate 0.5% at every step
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              The performance of such agents also changes with different activation functions used within the neural network, and we experiment with ReLU 
              (used in all experiments above), GeLU and Tanh. GeLU exhibits the strongest performance, being a stronger alternative to ReLU in most cases,
              due to preventing neurons from "dying out".
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/ddqn_tanh_cartpole1.png" class="images">
                  <figcaption>
                    Figure 7: Double DQN using Tanh nonlinearity
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/ddqn_gelu_cartpolev1.png" class="images">
                  <figcaption>
                    Figure 8: Double DQN using GELU nonlinearity
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              Finally, we test the performance of the DynaDQN algorithm. This is another augmentation of the DQN algorithm, where this time we 
              simultaneously build a model of the environment internally, which we can use to plan ahead. What this means is past observations are used 
              to train an internal world model which tries to learn - in this case - a mapping from (state, action) to next state, reward. By doing this, we
              can condense our past knowledge into a model which can be used to generate additional fabricated observations which can be used to supplement 
              actual observations. Every training step that uses these generated observations are called planning steps, and in general, this allows the DynaDQN
              to learn the optimal policy from fewer interactions with the environment. This can be useful when interactions with the world is costly. For the world 
              model we use a random forest regressor.
              <br></br>
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/dyna_dqn_algorithm.png" class="images">
                  <figcaption>
                    Figure 9: Dyna Q-Learning algorithm
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/dyna_random_forest_planning_3_gelu.png" class="images">
                  <figcaption>
                    Figure 10: Dyna DQN using Random forest as world model (3 planning steps)
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              Using 3 planning steps for each actual observation, we can observe that the DynaDQN algorithm training appears more stable and converges to the 
              optimal policy in less episodes compared to all previous models. Note that the "quality" of such fabricated observations are lower than actual observations 
              (due to innaccuracies in the world model). So it is important to ensure that not too many planning steps are used.
              <br></br>
              The last model I implemented is the Dyna Q+ algorithm, which records the last time an action was chosen in a particular state, and uses this to 
              encourage exploration over time of under trialled actions. This is especially powerful for changing environments as the algorithm is 
              more inclined to explore new actions if they haven't been used in a while. For this particular task, which uses a static environment, the extra exploration 
              provides no additional benefit. In fact, the extra exploration of potentially bad actions means that the algorithm takes more episodes to converge to the 
              optimal policy.
            </p>
            <div>
              <figure>
                <img src="images/dqn/dyna_dqn+_planning_3.png" class="images">
                <figcaption>
                  Figure 11: Dyna DQN+
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
        <div>
          <button type="button" class="sub-collapsible text">Trading Stocks</button>
          <p class="hidden text paragraph">
            For Trading stocks, the DQN algorithm was trialled using the gym trading environment. We omit the discussion of any Dyna models in this section 
            as it is well known that modelling stock data is a difficult problem, let alone predicting how each of the individual features transition (modelling
            state to state transitions).
            <br></br>
            These trading gym environments use the exact same methods as the regular 
            gym environments, but use a user defined dataframe which contains price information of a particular stock. For this task, the data is preprocessed to 
            include information on some of the most common financial indicators, such as simple moving averages, exponential moving averages, RSI and the MACD. These 
            are presented alongside price information to the agent, which returns either 1 for buy, 0 for neutral position and -1 for sell. Using the premade 
            gym environment makes it easy to train on multiple stocks. In this example we try training a DQN algorithm on AAPL, AMZN and GOOG stock prices for the last 
            1000 days or roughly three years. The results show that it is not trivial to use a RL agent to trade on regular price data. The quality of the data, the changing 
            distribution of data, as well as the low signal to noise ratio makes it very difficult to ascertain any concrete patterns in the data. We can see that over time 
            as the model trains, the performance (portfolio returns over market) still remains very low.
            <br></br>
            <img src="images/dqn/portfoliovsmarketreturns.png" class="images">
            <br></br>
          </p>
        </div>
      </div>
    </div>
  
    <div class="center project">
      <button type="button" class="collapsible text">Login App</button>

      <div class="hidden project">

        <div>
          <button type="button" class="sub-collapsible text">Background</button>
          <p class="hidden text paragraph">The Login App is the first "proper" purely personal project I have taken on. The main objective of this
            project was to improve my ability of writing Object-Oriented code. Starting off, I had a basic idea of what I wanted to build -
            knowing that along the way I would find more and more features to add and improve on. I chose to use Python, due to it being my
            strongest and most used programming language thus far.
            <br></br>
            I wrote all of the code (which can be found <a href="https://github.com/yuwei-1/login_app" target="_blank">here</a>) using the
            AGILE development method - a snippet of my sprint logs is shown below.
            <br></br>
            <img src="images/Login_app/agile.jpg" class="images">
          </p>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">GUI</button>
          <p class="hidden text paragraph"> The GUI I used for my App was created using a framework built in to the standard library provided
            by Python - Tkinter. Using Tkinter sometimes gives the UI a very outdated/clunky look, however, it is easy for new users to pick
            up in a short time. The main page, a Tk() object, is the login page (shown below), where existing users can login in. The other pages used by
            the app open as Toplevel() objects that "stem" from the login page.
            <br></br>
            <img src="images/Login_app/GUI1.jpg" class="images">
            <br></br>
            If you are not an existing user, there is an option to register an account with the app. After choosing this option, the user will be asked
            to provide several details: a new username, password and email address.
            <br></br>
            <img src="images/Login_app/GUI2.jpg" class="images">
            <br></br>
            Upon successful registration, an email will be sent to your email address, and a success message will be displayed to the user.
            The user can now proceed to log in to the app.
            <br></br>
            <img src="images/Login_app/GUI3.jpg" class="images">
            <br></br>
            <img src="images/Login_app/GUI4.jpg" class="images">
            <br></br>
            If the username and password both match an account on the system, the app will then prompt the user for a 6 digit code, sent to the
            email address they signed up with. (2FA process described in next section)
            <br></br>
            <img src="images/Login_app/GUI5.jpg" class="images">
            <br></br>
            <img src="images/Login_app/GUI6.jpg" class="images">
            <br></br>
            If the user enters the code correctly, they will reach the landing page of the login app. Here, their username and email address will
            displayed. The background is a reference to the TV show Mr. Robot.
            <br></br>
            <img src="images/Login_app/GUI7.jpg" class="images">
            </address>
            </thead>
          </p>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">2FA and Security</button>

          <div class="hidden">
            <p class="text paragraph">This section will talk about the 2 Factor Authentication (2FA) used within the app and the basic security
            measures used to make the app more secure, and to protect users. The entire process is very simple, and starts off by concatenating 6 random
            integers into a code and sending this to the user's email address, whilst simultaneously prompting the user to enter the aforementioned code.
            Then, these can simply be compared - resulting in successful login if they are same, and rejecting sign in if otherwise. In order to add
            2FA to the app, discovery work was done into sending emails using the built in Python library, smtplib. In particular, I used the class
            smtplib.SMTP_SSL(); the code for the mail sending function is shown below.
            <br></br>
            <img src="images/Login_app/2FA1.jpg" class="images">
            <br></br>
            I will now briefly describe the process carried out by this code.
            <br>
            A link is established to a Simple Mail Transfer Protocol (SMTP) server, here the
            specific one used is owned by Google, the address being: "smtp.gmail.com". After using method .login(), and providing correct details to a valid
            gmail account, an email can be sent. The email and other details, such as recipient's address is sent over to the host address corresponding to the
            SMTP server. The SMTP server will then contact the DNS server in order to find the recipient's IP address.The SMTP server finally sends the information
            to email recipient's Mail Transfer Agent (MTA) server. Then, using either POP or IMAP protocols (depending on recipients email client) the email
            will arrive at the recipient's inbox. The whole process is done using Secure Sockets Layer (SSL) protocol, which creates an encrypted
            connection, making sure that in the unlikely case the email is intercepted midway, hackers cannot access the information within the email.
            <br></br>
            Aside from 2FA, the app requires users to create a password that adheres to standard requirements:
            </p>

            <ul class="paragraph combine">
              <li> Password must be at least 8 characters in length.</li>
              <li> Password must contain a Capital letter.</li>
              <li> Password must contain a special character.</li>
              <li> Password must contain a number.</li>
            </ul>
            <p class="text paragraph combine">
            If the user fails to meet these requirements, the app will reject the password and prompt the user to strengthen the password.
            <br></br>
            <img src="images/Login_app/security1.jpg" class="images">
            <br></br>
            Additionally, passwords that are stored on the database are hashed using SHA-256. This adds extra steps for a hacker that gets access
            to the database. If they try to brute force the password, they will need to apply the hash every attempt in order to find the correct
            password, which is more time consuming. The algorithm is described in more detail in the next section.
            </p>
          </div>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">SHA-256 Algorithm</button>
          <p class="hidden text paragraph">This section will briefly go over the Secure Hashing Algorithm(SHA) - 256, implemented from scratch to be
            used to hash passwords in the Login app. Information in this section can be found in its entirety
            <a href="https://en.wikipedia.org/wiki/SHA-2" target="_blank">here</a>, in the pseudocode section.
            <br></br>
            Below, you can see the main method implemented in my class, SHA2, called .encrypt().
            <br></br>
            <img src="images/Login_app/SHA21.jpg" class="images">
            <br></br>
            Walking through the steps within this method, first there is a method called binary conversion. This simply takes the password string,
            and converts each character to binary. Next, the binary string is "padded" with zeros and the message length (also in binary) so that
            we end up with a 512-bit binary string.
            <br></br>
            The next two steps are very related, so they shall be described together. In SHA-256 hashing, there are 64 so called W values, which
            are used in "rounds". The first 16 W values, labelled 0 to 15, are created using your initial message, splitting up the 512-bits into
            16 x 32-bit chunks. Then, after these have been obtained, the algorithm will proceed to iterate 64 minus 16 times to "generate" the rest
            of the W values. This is using the formula below:
            <br></br>
            <img src="images/Login_app/SHA22.jpg" class="images">
            <br></br>
            Which uses functions for right rotate (switching order of bits) and right shift (removing bits and padding with zeros), as well as existing
            W values to find new W values. After this is completed, 64 "rounds" will commence.
            <br></br>
            The diagram of a single round is shown below.
            <br></br>
            <img src="images/Login_app/SHA23.png" class="images">
            <br></br>
            A to H at the top represent "working variables". Using a number of different functions: maj, conditional, Sigma, combined with our W values
            and K values (created using cube roots of first 64 prime numbers) using modular addition, these working variables are then used to initialize
            the working variables of the next round. This is repeated 64 times to retrieve the final set of working variables. Adding these to the initial
            working variables and converting to hexadecimal, and finally concatentating, we retrieve the SHA256 hash.
            <br></br>
            Unittests were written to test the validity of hash function.
          </p>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">SQLite3 Database</button>
          <p class="hidden text paragraph">SQLite3, built - in to the standard python library was used to connect/create/maintain the database for the Login app.
            Queries to the database were written in SQL.
          </p>
        </div>

      </div>

    </div>

    <div class="center project">
      <button type="button" class="collapsible text">NoVa experiment (Computer Vision)</button>
      <div class="hidden project">
        <div>
          <button type="button" class="sub-collapsible text">Background</button>
          <p class="hidden text paragraph">This project was completed as part of an assignment for one of my final year modules, "Machine Learning
            for Physicists". The final grade achieved for this module was 82%. The objective of this project was to explore methods for classifying
            detector images. Traditionally, detector images are pored over by teams of scientists at particle colliders in order to determine what
            useful information can be extracted. However, as technology has improved, detectors spit out thousands, if not, millions of images per day.
            This is when it becomes unfeasible to check each image individually. So this is where machine learning can come in handy.
            <br></br>
            My exploration of the different tools available to handle this task were documentated, and presented in the form of a scientific paper. I have
            attached the final paper below for you to peruse:
            <br></br>
            <iframe src="documents/neutrino_oscillation_report.pdf" class="pdf"></iframe>
          </p>
        </div>
      </div>
    </div>

    <div class="center project">
      <button type="button" class="collapsible text">NLP project</button>
      <p class="hidden text paragraph">Coming soon</p>
    </div>


    <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }

    var coll = document.getElementsByClassName("sub-collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("sub-active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }
  </script>



</body>
</html>
