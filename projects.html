<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="stylesheet" type="text/css" href="style/style.css">
    <meta charset="UTF-8">
    <div class="navbar">
      <a href="index.html" target="_self">Home</a>
      <a href="projects.html" target="_self">Projects</a>
    </div>
    <title>Yuwei Zhu</title>
</head>

<body id="backdrop">
  <h1 class="text" id="main-title">PERSONAL PROJECTS</h1>
    <div class="center project top">
      <button type="button" class="collapsible text">Reinforcement Learning for Market Prediction</button>
      <div class="hidden project">
        <div>
          <button type="button" class="sub-collapsible text">Initial Work</button>
          <div class="hidden text paragraph margintop">
            <p>
              One possible strategy for trading the financial markets is utilising reinforcement learning algorithms, and frame the
              trading task as a "game", where the agent collects rewards for good trades and negative rewards for bad trades.
              This particular personal project does a deep dive into common reinforcement learning strategies, before
              applying them to the stock market.
              <br></br>
              Before diving straight into a trading environment, we will sanity check our implementation and tune the function approximator (neural network)
              using the well known cartpole game provided by OpenAI Gym/Gymnasium (fig. 1). The code used in this project can be found 
              <a href="https://github.com/yuwei-1/Market-Prediction-Research" target="_blank">here</a>. 
              Included in the repository is a full set of python unittests which check basic functionality of each of the implemented models to ensure
              that the model is working as intended. 
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/env.png" class="images">
                  <figcaption>
                    Figure 1: Cartpole environment
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/dqn_algorithm.png" class="images">
                  <figcaption>
                    Figure 2: Description of the DQN algorithm
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              <br></br>
              We will be utilizing the Q-learning algorithm (fig. 2), which is an off-policy algorithm (which means it chooses actions in 
              a different manner compared to the policy the agent is trying to learn) which uses a temporal difference TD(0) update.
              When using a neural network to approximate the action-value function (usually denoted with Q(s,a)), the network is 
              commonly referred to as DQN. The DQN can be augmented in certain ways to produce other variants which can excel under
              certain circumstances; namely Double DQN, DynaQ, DynaQ+ and even dueling DQNs (which will not be covered in this project). 
              We will be testing the efficacy of these models on the aforementioned environment, before eventually moving to predicting optimal trades. 

              The DQN itself is a very powerful model which was demonstrated to play many Atari games with good results and even achieving super human play
              in some games. The Q-learning algorithm can be applied to both tabular and continuous environments, by estimating a value for each possible
              action and state combo. The Q-learning update is mathematically proven to converge to the optimal policy under certain conditions; namely that
              each state is visited an infinite number of times. However, as we can only train for finite time periods, sometimes the Q-learning agent
              can get stuck in a local minima, and one commonly known issue is overestimation. In fact, when performing a deep dive on the DQN using a 
              basic toy environment which always remains in the same state, with two possible actions, 0 and 1 - where 1 always yields a reward of 1, whereas
              the action zero permanently grants zero reward, I observed that the estimated value for action zero often gets overestimated during training, despite
              the model never encountering a reward for that action; this is because of the greedy term which takes the maximum value of the value function for the 
              next state.
              <br></br>
              Here we show the reward versus time plots for the DQN itself, and it can be seen that the model is highly unstable and struggles to reach
              the optimal policy, even after 5000 episodes of training.
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/single_dqn_cartpole.png" class="images">
                  <figcaption>
                    Figure 3: single DQN run 1
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/single_dqn_cartpole_ours.png" class="images">
                  <figcaption>
                    Figure 4: single DQN run 2
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              We can augment the DQN model such that we duplicate the Q-network to form a policy network and a target network. The intuition behind this 
              construction is to mitigate the overestimation problem and the "moving target" problem. By fixing the one network and updating the other, we can fix the policy while updating
              which makes the training much more stable. Obviously the fixed network prediction will be bad initially, but it is updated periodically to match
              the network that is being updated. Additionally, we experiment with hard and soft updates, which are complete updates at intervals, or small 
              interpolations at each time step.
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/double_dqn_hard_cartpole_ours.png" class="images">
                  <figcaption>
                    Figure 5: Double DQN with hard updates every 500 steps
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/double_dqn_cartpole_ours.png" class="images">
                  <figcaption>
                    Figure 6: Double DQN with soft updates - interpolate 0.5% at every step
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              The performance of such agents also changes with different activation functions used within the neural network, and we experiment with ReLU 
              (used in all experiments above), GeLU and Tanh. GeLU exhibits the strongest performance, being a stronger alternative to ReLU in most cases,
              due to preventing neurons from "dying out".
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/ddqn_tanh_cartpole1.png" class="images">
                  <figcaption>
                    Figure 7: Double DQN using Tanh nonlinearity
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/ddqn_gelu_cartpolev1.png" class="images">
                  <figcaption>
                    Figure 8: Double DQN using GELU nonlinearity
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              Finally, we test the performance of the DynaDQN algorithm. This is another augmentation of the DQN algorithm, where this time we 
              simultaneously build a model of the environment internally, which we can use to plan ahead. What this means is past observations are used 
              to train an internal world model which tries to learn - in this case - a mapping from (state, action) to next state, reward. By doing this, we
              can condense our past knowledge into a model which can be used to generate additional fabricated observations which can be used to supplement 
              actual observations. Every training step that uses these generated observations are called planning steps, and in general, this allows the DynaDQN
              to learn the optimal policy from fewer interactions with the environment. This can be useful when interactions with the world is costly. For the world 
              model we use a random forest regressor.
              <br></br>
            </p>
            <div class="row">
              <div class="column">
                <figure>
                  <img src="images/dqn/dyna_dqn_algorithm.png" class="images">
                  <figcaption>
                    Figure 9: Dyna Q-Learning algorithm
                  </figcaption>
                </figure>
              </div>
              <div class="column">
                <figure>
                  <img src="images/dqn/dyna_random_forest_planning_3_gelu.png" class="images">
                  <figcaption>
                    Figure 10: Dyna DQN using Random forest as world model (3 planning steps)
                  </figcaption>
                </figure>
              </div>
            </div>
            <p>
              Using 3 planning steps for each actual observation, we can observe that the DynaDQN algorithm training appears more stable and converges to the 
              optimal policy in less episodes compared to all previous models. Note that the "quality" of such fabricated observations are lower than actual observations 
              (due to innaccuracies in the world model). So it is important to ensure that not too many planning steps are used.
              <br></br>
              The last model I implemented is the Dyna Q+ algorithm, which records the last time an action was chosen in a particular state, and uses this to 
              encourage exploration over time of under trialled actions. This is especially powerful for changing environments as the algorithm is 
              more inclined to explore new actions if they haven't been used in a while. For this particular task, which uses a static environment, the extra exploration 
              provides no additional benefit. In fact, the extra exploration of potentially bad actions means that the algorithm takes more episodes to converge to the 
              optimal policy.
            </p>
            <div>
              <figure>
                <img src="images/dqn/dyna_dqn+_planning_3.png" class="single_images">
                <figcaption>
                  Figure 11: Dyna DQN+
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
        <div>
          <button type="button" class="sub-collapsible text">Trading Stocks</button>
          <p class="hidden text paragraph">
            For Trading stocks, the DQN algorithm was trialled using the gym trading environment. We omit the discussion of any Dyna models in this section 
            as it is well known that modelling stock data is a difficult problem, let alone predicting how each of the individual features transition (modelling
            state to state transitions).
            <br></br>
            These trading gym environments use the exact same methods as the regular 
            gym environments, but use a user defined dataframe which contains price information of a particular stock. For this task, the data is preprocessed to 
            include information on some of the most common financial indicators, such as simple moving averages, exponential moving averages, RSI and the MACD. These 
            are presented alongside price information to the agent, which returns either 1 for buy, 0 for neutral position and -1 for sell. Using the premade 
            gym environment makes it easy to train on multiple stocks. In this example we try training a DQN algorithm on AAPL, AMZN and GOOG stock prices for the last 
            1000 days or roughly three years. The results show that it is not trivial to use a RL agent to trade on regular price data. The quality of the data, the changing 
            distribution of data, as well as the low signal to noise ratio makes it very difficult to ascertain any concrete patterns in the data. We can see that over time 
            as the model trains, the performance (portfolio returns over market) still remains very low.
            <br></br>
            <img src="images/dqn/portfoliovsmarketreturns.png" class="single_images">
            <br></br>
          </p>
        </div>
      </div>
    </div>

    <div class="center project">
      <button type="button" class="collapsible text">Thesis: Short-Term Plasticity as an improved model of Linear Transformers</button>
        <div class="hidden project">
          <div>
            <button type="button" class="sub-collapsible text">Resources</button>
      
            <div class="hidden text paragraph margintop">
                <p>
                  For this research project, I was partnered with the Huawei Neuromorphic Computing Group in Zurich. I was granted use of the Huawei Computing cluster. 
                  Workloads were managed using Simple Linux Utility for Resource Management (SLURM). Additionally, in order to train models 
                  overnight/long durations, it was essential to use tmux (terminal multiplexer) which kept programs running regardless of whether the local machine is 
                  connected or not. In order to utilize the maximum potential of the nodes, it was important to become acquainted with these esoteric systems.
                  Almost all models run in the experiments in this project were created by myself using Pytorch. Public implementations were only used where necessary to verify 
                  my own implementation and results. This was a conscious choice to embrace every learning opportunity possible.
                </p>
              </div>

            <button type="button" class="sub-collapsible text">Paper</button>
      
            <div class="hidden text paragraph margintop">
                <p>
                  <b>Abstract: </b>
                  <br></br>
                  It has long been known that Short-Term memory is a key component of the human brain, used for learning skills for one-time or periodic use. This skill is also useful 
                  in language modelling, as oftentimes themes or topics of a corpus of text appear and reappear throughout an article, necessitating the need for the element of forgetting 
                  in language models, which is not currently present. By utilizing ideas presented in recent works about the Short-Term Plasticity Neuron (STPN) and the Linearized Transformer, 
                  we present a new language model, named the STPN-transformer (STPNt), which builds on the idea of the Linearized Transformer by augmenting it with learning to learn and forget. 
                  Experimental results demonstrate the advantages of the STPNt over the Linear Transformer in limited memory settings on the Wikitext-2 dataset. Our research in this thesis begins 
                  to demonstrate some of the potential advantages that this type of learning can provide in language modelling, to be built on in future works.
                  <br></br>

                  <b>Contributions to field: </b>
                  <ul>
                    <li>Propose three variants of a novel architecture named the STPNt (Short-Term Plasticity Transformer), which applies a STPN-like mechanism to the Linear Transformer attention formulation.</li>
                    <li>Show that the STPNt model can learn proficiently and show advantages on a toy problem: The Associative Retrieval Task (ART)</li>
                    <li>Introduce a new variant of the ART, named AdaptART, which is more complex and further differentiates the performance of models with inherent forgetting mechanisms.</li>
                    <li>Propose a biologically plausible practical addition to the original STPN mechanism, which is shown to stabilize training and improve performance, for the Short-Term Plasticity-RNN 
                      (STPNr) as well as our model, the STPNt.</li>
                    <li>Show experimental results on the Wikitext-2 dataset that demonstrates that the STPNt provides advantages over the Linear Transformer in limited memory settings.</li>
                  </ul>
                  
                  <br></br>
                  <b>Summary of Results, Conclusion and Further Work: </b>
                  <br></br>
                  In this work, we present three models: STPNt, STPNt with context, and the STPNt-LSTM, all adapted from the linearized Transformer model. The Linear Transformer provides linear scaling complexity with 
                  respect to sequence length, but does this by approximating the softmax activation in the Transformer, naturally leading to a performance decrease. The attention layer in the Linear Transformer maintains a 
                  cumulative matrix which retains information from each time step. Due to the purely additive updates at each time step, the entries of this matrix increase unbounded over time, which hinders performance on 
                  long training and evaluation sequences. The models we introduce apply the idea of fast weight programming, where certain network parameters are trained in order to modulate what information to retain and 
                  forget at each time step. The three variants propose three different methods of modulation:

                  <ul>
                    <li>The STPNt modulates in a constant manner. This network learns a constant decay parameter which applies at every time step. Therefore, this model learns an effective context window from the training data. 
                      We refer to this model as the STPNt or basic STPNt.</li>
                    <li>The STPNt with context modulates based on the current token only. Thus, this model learns which specific tokens are more or less useful, as well as particular tokens that may indicate the start of a new 
                      topic where the model could reset its current state.</li>
                    <li>The STPNt-LSTM utilizes a LSTM cell, which collates contextual information over many time steps, and utilizes the output of this cell to determine suitable checkpoints to reset its context, as well as determine 
                      which tokens are more or less useful based on past context.</li>
                  </ul>
                  Since the both the Linear Transformer and STPNt variants approximates the Transformer attention, there is an inherent decrease in performance. For this reason, in our results, we mostly keep the discussion between 
                  our models and the Linear Transformer instead of comparing with the Transformer itself.
                </p>
                <div>
                  <figure>
                    <img src="images/thesis/summary.png" class="single_images">
                    <figcaption>
                      Figure 1: (Left half): Toy problem results; Associative Retrieval Task with replacement, (Top Right): Toy problem results; Adaptive ART, (Bottom Right): War and Peace results.
                    </figcaption>
                  </figure>
                </div>
                <p>
                  Experimentation was conducted on several toy problems and two datasets: the novel War and Peace, and the language modelling dataset Wikitext-2. In the toy problem, the basic 
                  STPNt and STPNt with context were benchmarked against similar models in the literature, as well as the Transformer and Linear Transformer. The results presented in fig. 1 show that the STPNt outperforms the majority 
                  of models on both tasks, but performs worse on the toy problem designed to better mimic retrieval in a natural language context. The STPNt with context was introduced here, and showed superior performance to the basic STPNt. 
                  This discussion was extended to a language corpus, the War and Peace novel, which recapitulated the importance of context, with the contextual STPNt greatly outperforming the basic model. The experimentation on War and Peace 
                  also revealed another insight: stacking STPNt attention blocks caused training instabilities - instead we discovered that the optimal network architecture involved a single STPNt block stacked with Linear Transformer blocks. 
                  Finally, experimentation was conducted using the Wikitext-2 dataset, where we also introduced the final model variant: STPNt-LSTM.
                </p>
                <div>
                  <figure>
                    <img src="images/thesis/testpplseq.png" class="single_images">
                    <figcaption>
                      Figure 2: Test perplexities for different sequence lengths of training and evaluation
                    </figcaption>
                  </figure>
                </div>
                <p>
                  During these experiments, we varied the training and evaluation sequence lengths, and compared the perplexity of the trained models on the test set, shown in figure 3. Surprisingly, the STPNt with context 
                  did not provide much benefit over the Linear Transformer, and in fact showed signs of overfitting; lower validation perplexity than the Linear Transformer but higher test perplexity. Nonetheless, the STPNt with context did 
                  outperform on sequence lengths beyond 256, and this could be an avenue for further work. The STPNt-LSTM yielded much more positive results: consistently lower training and validation perplexity compared to the Linear Transformer, 
                  and demonstrated the ability to utilize longer sequence lengths compared to the Linear Transformer and even the Transformer itself. In fact, the STPNt-LSTM was the only model to achieve a test perplexity improvement from increasing 
                  the sequence length from 64 to 128. Knowing that the LSTM cell endows the model with the ability to learn longer sequences, future work could explore the use of an even larger cell state, to see if this could improve things further.
                </p>
                <div>
                  <figure>
                    <img src="images/thesis/ppl_results_table.png" class="single_images">
                    <figcaption>
                      Figure 3: Wikitext-2 best test and validation perplexities for different training/evaluation sequence lengths
                    </figcaption>
                  </figure>
                </div>
                <p>
                  <b>Plastic Parameter Analysis: </b>
                  <br></br>
                  We try to analyse what the STPNt-LSTM is learning by understanding what plastic parameters (lambda, gamma) the model uses when evaluating a a short sequence of text. We include this section to help improve the interpretability of the model, 
                  in the same vein as work done by Kaparthy et al. Using some sample text, we compare the lambda (assigned to modulate the impact of the previous information) 
                  and gamma (modulates the impact of current token) for each token in the sequence.
                </p>
                <div>
                  <figure>
                    <img src="images/thesis/plasticparams.png" class="single_images">
                    <figcaption>
                      Figure 4: Analysis of STPNt-LSTM weights
                    </figcaption>
                  </figure>
                </div>
                <p>
                  Through analyzing the results, shown in fig. 4, we see that in fact the lambda parameters are almost unused; for most timesteps, the value of lambda is very close to one, which indicates an incredibly slow decay rate, 
                  which allows for long context. The model does not reset the sequence at any point, but since the sequence is only 128 tokens long, additional analysis needs to be done on longer text, specifically between different topics and articles. 
                  One token that seems to encourage more forgetting of the past are quotation marks, shown by the small dips in the lambda graph. What is more surprising is the model's use of gamma parameters. Here we see that the tokens that the model retains 
                  the most correspond to tokens such as full stops, "in", "as" which are words that appear generally in text, not providing much information about the subsequent tokens. This may be the case due to the small dataset size of Wikitext-2. The model 
                  may not have encountered some words such as names - "Josh" many times, thus not encoding much information in the embeddings of those words, rendering them less useful. However, this is just conjecture, and more work must be done, training on 
                  larger corpuses, such as Wikitext, or the 1 billion words corpus.
                  <br></br>
                  We finish our summary by noting several avenues for the continuation of the work in this thesis. Firstly, the models used in the experiments are relatively small compared to those in the literature, and the performance statistics listed 
                  are far from being state-of-the-art, even among non pre-trained models. Thus, one important next step would be to scale up our models, preferably to around 24M trainable parameters which is comparable to the literature, and incorporate 
                  any important regularization techniques. This will be crucial for determining whether our model can truly deliver some benefit. Furthermore, it was discovered upon examining our models parameters during evaluation that the STPNt-LSTM is 
                  learning some counter-intuitive properties of the data, and further work should be done training it on larger language corpuses to explore the consequences of this. Finally, by changing our embedding scheme to a relative one, we could 
                  utilize a "cache" during evaluation time, and this could have interesting effects on the STPNt performance.
                </p>

              </div>
          
          </div>
        </div>
    </div>
  
    <div class="center project">
      <button type="button" class="collapsible text">Login App</button>

      <div class="hidden project">

        <div>
          <button type="button" class="sub-collapsible text">Background</button>
          <p class="hidden text paragraph">The Login App is the first "proper" purely personal project I have taken on. The main objective of this
            project was to improve my ability of writing Object-Oriented code. Starting off, I had a basic idea of what I wanted to build -
            knowing that along the way I would find more and more features to add and improve on. I chose to use Python, due to it being my
            strongest and most used programming language thus far.
            <br></br>
            I wrote all of the code (which can be found <a href="https://github.com/yuwei-1/login_app" target="_blank">here</a>) using the
            AGILE development method - a snippet of my sprint logs is shown below.
            <br></br>
            <img src="images/Login_app/agile.jpg" class="images">
          </p>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">GUI</button>
          <p class="hidden text paragraph"> The GUI I used for my App was created using a framework built in to the standard library provided
            by Python - Tkinter. Using Tkinter sometimes gives the UI a very outdated/clunky look, however, it is easy for new users to pick
            up in a short time. The main page, a Tk() object, is the login page (shown below), where existing users can login in. The other pages used by
            the app open as Toplevel() objects that "stem" from the login page.
            <br></br>
            <img src="images/Login_app/GUI1.jpg" class="images">
            <br></br>
            If you are not an existing user, there is an option to register an account with the app. After choosing this option, the user will be asked
            to provide several details: a new username, password and email address.
            <br></br>
            <img src="images/Login_app/GUI2.jpg" class="images">
            <br></br>
            Upon successful registration, an email will be sent to your email address, and a success message will be displayed to the user.
            The user can now proceed to log in to the app.
            <br></br>
            <img src="images/Login_app/GUI3.jpg" class="images">
            <br></br>
            <img src="images/Login_app/GUI4.jpg" class="images">
            <br></br>
            If the username and password both match an account on the system, the app will then prompt the user for a 6 digit code, sent to the
            email address they signed up with. (2FA process described in next section)
            <br></br>
            <img src="images/Login_app/GUI5.jpg" class="images">
            <br></br>
            <img src="images/Login_app/GUI6.jpg" class="images">
            <br></br>
            If the user enters the code correctly, they will reach the landing page of the login app. Here, their username and email address will
            displayed. The background is a reference to the TV show Mr. Robot.
            <br></br>
            <img src="images/Login_app/GUI7.jpg" class="images">
            </address>
            </thead>
          </p>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">2FA and Security</button>

          <div class="hidden">
            <p class="text paragraph">This section will talk about the 2 Factor Authentication (2FA) used within the app and the basic security
            measures used to make the app more secure, and to protect users. The entire process is very simple, and starts off by concatenating 6 random
            integers into a code and sending this to the user's email address, whilst simultaneously prompting the user to enter the aforementioned code.
            Then, these can simply be compared - resulting in successful login if they are same, and rejecting sign in if otherwise. In order to add
            2FA to the app, discovery work was done into sending emails using the built in Python library, smtplib. In particular, I used the class
            smtplib.SMTP_SSL(); the code for the mail sending function is shown below.
            <br></br>
            <img src="images/Login_app/2FA1.jpg" class="images">
            <br></br>
            I will now briefly describe the process carried out by this code.
            <br>
            A link is established to a Simple Mail Transfer Protocol (SMTP) server, here the
            specific one used is owned by Google, the address being: "smtp.gmail.com". After using method .login(), and providing correct details to a valid
            gmail account, an email can be sent. The email and other details, such as recipient's address is sent over to the host address corresponding to the
            SMTP server. The SMTP server will then contact the DNS server in order to find the recipient's IP address.The SMTP server finally sends the information
            to email recipient's Mail Transfer Agent (MTA) server. Then, using either POP or IMAP protocols (depending on recipients email client) the email
            will arrive at the recipient's inbox. The whole process is done using Secure Sockets Layer (SSL) protocol, which creates an encrypted
            connection, making sure that in the unlikely case the email is intercepted midway, hackers cannot access the information within the email.
            <br></br>
            Aside from 2FA, the app requires users to create a password that adheres to standard requirements:
            </p>

            <ul class="paragraph combine">
              <li> Password must be at least 8 characters in length.</li>
              <li> Password must contain a Capital letter.</li>
              <li> Password must contain a special character.</li>
              <li> Password must contain a number.</li>
            </ul>
            <p class="text paragraph combine">
            If the user fails to meet these requirements, the app will reject the password and prompt the user to strengthen the password.
            <br></br>
            <img src="images/Login_app/security1.jpg" class="images">
            <br></br>
            Additionally, passwords that are stored on the database are hashed using SHA-256. This adds extra steps for a hacker that gets access
            to the database. If they try to brute force the password, they will need to apply the hash every attempt in order to find the correct
            password, which is more time consuming. The algorithm is described in more detail in the next section.
            </p>
          </div>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">SHA-256 Algorithm</button>
          <p class="hidden text paragraph">This section will briefly go over the Secure Hashing Algorithm(SHA) - 256, implemented from scratch to be
            used to hash passwords in the Login app. Information in this section can be found in its entirety
            <a href="https://en.wikipedia.org/wiki/SHA-2" target="_blank">here</a>, in the pseudocode section.
            <br></br>
            Below, you can see the main method implemented in my class, SHA2, called .encrypt().
            <br></br>
            <img src="images/Login_app/SHA21.jpg" class="images">
            <br></br>
            Walking through the steps within this method, first there is a method called binary conversion. This simply takes the password string,
            and converts each character to binary. Next, the binary string is "padded" with zeros and the message length (also in binary) so that
            we end up with a 512-bit binary string.
            <br></br>
            The next two steps are very related, so they shall be described together. In SHA-256 hashing, there are 64 so called W values, which
            are used in "rounds". The first 16 W values, labelled 0 to 15, are created using your initial message, splitting up the 512-bits into
            16 x 32-bit chunks. Then, after these have been obtained, the algorithm will proceed to iterate 64 minus 16 times to "generate" the rest
            of the W values. This is using the formula below:
            <br></br>
            <img src="images/Login_app/SHA22.jpg" class="images">
            <br></br>
            Which uses functions for right rotate (switching order of bits) and right shift (removing bits and padding with zeros), as well as existing
            W values to find new W values. After this is completed, 64 "rounds" will commence.
            <br></br>
            The diagram of a single round is shown below.
            <br></br>
            <img src="images/Login_app/SHA23.png" class="images">
            <br></br>
            A to H at the top represent "working variables". Using a number of different functions: maj, conditional, Sigma, combined with our W values
            and K values (created using cube roots of first 64 prime numbers) using modular addition, these working variables are then used to initialize
            the working variables of the next round. This is repeated 64 times to retrieve the final set of working variables. Adding these to the initial
            working variables and converting to hexadecimal, and finally concatentating, we retrieve the SHA256 hash.
            <br></br>
            Unittests were written to test the validity of hash function.
          </p>
        </div>

        <div>
          <button type="button" class="sub-collapsible text">SQLite3 Database</button>
          <p class="hidden text paragraph">SQLite3, built - in to the standard python library was used to connect/create/maintain the database for the Login app.
            Queries to the database were written in SQL.
          </p>
        </div>

      </div>

    </div>

    <div class="center project">
      <button type="button" class="collapsible text">NoVa experiment (Computer Vision)</button>
      <div class="hidden project">
        <div>
          <button type="button" class="sub-collapsible text">Background</button>
          <p class="hidden text paragraph">This project was completed as part of an assignment for one of my final year modules, "Machine Learning
            for Physicists". The final grade achieved for this module was 82%. The objective of this project was to explore methods for classifying
            detector images. Traditionally, detector images are pored over by teams of scientists at particle colliders in order to determine what
            useful information can be extracted. However, as technology has improved, detectors spit out thousands, if not, millions of images per day.
            This is when it becomes unfeasible to check each image individually. So this is where machine learning can come in handy.
            <br></br>
            My exploration of the different tools available to handle this task were documentated, and presented in the form of a scientific paper. I have
            attached the final paper below for you to peruse:
            <br></br>
            <iframe src="documents/neutrino_oscillation_report.pdf" class="pdf"></iframe>
          </p>
        </div>
      </div>
    </div>

    <div class="center project">
      <button type="button" class="collapsible text">NLP project</button>
      <p class="hidden text paragraph">Coming soon</p>
    </div>


    <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }

    var coll = document.getElementsByClassName("sub-collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("sub-active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }
  </script>



</body>
</html>
